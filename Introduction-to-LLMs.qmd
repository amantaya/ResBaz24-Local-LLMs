---
title: "Introduction to Running LLMs - Locally!"
author: "Andrew Antaya"
format: revealjs
---

## What are Large Language Models?

-   Large Language Models (LLMs) are a subset of **Deep Learning**,
    which is a type of **Machine Learning**.

## Machine Learning vs. Artificial Intelligence?

-   ***Machine Learning*** is sometimes used interchangeably with the
    term ***Artificial Intelligence*** **(AI)**, however machine
    learning typically refers to algorithms that can learn patterns from
    data and make predictions based on that data.

## Machine Learning vs. Artificial Intelligence?

-   ***Machine Learning*** is sometimes used interchangeably with the
    term ***Artificial Intelligence*** **(AI)**, however machine
    learning typically refers to algorithms that can learn patterns from
    data and make predictions based on that data.
-   ***Machine Learning*** is a subset of **AI**, which is a broader
    field that includes other types of algorithms that can perform tasks
    that typically require human intelligence.

## What are Large Language Models?

-   LLMs refer to *general purpose* models that can be ***pre-trained***
    and the ***fine-tuned*** for specific tasks.

## Why LLMs Are a Hot Topic

-   Relatively new - GPT-1 (117 million parameters) was released in 2018

## Why LLMs Are a Hot Topic

-   Relatively new - GPT-1 (117 million parameters) was released in 2018
-   Perform well on wide-variety of language tasks that have
    traditionally been difficult for computers to solve (e.g. question
    answering, summarization, translation)

## Why LLMs Are a Hot Topic

-   Relatively new - GPT-1 (117 million parameters) was released in 2018
-   Perform well on wide-variety of language tasks that have
    traditionally been difficult for computers to solve (e.g. question
    answering, summarization, translation)
-   Can be fine-tuned to solve specific problems in different domains

## Why LLMs Are a Hot Topic

-   Relatively new - GPT-1 (117 million parameters) was released in 2018
-   Perform well on wide-variety of language tasks that have
    traditionally been difficult for computers to solve (e.g. question
    answering, summarization, translation)
-   Can be fine-tuned to solve specific problems in different domains
-   Recent software innovations (e.g., Transformers) and computational
    improvements have made training on vast data sets possible

## 3 Main Characteristics of LLMs

1.  Large
2.  General Purpose
3.  Pre-trained and Fine-tuned

## How "Large" are LLMs?

-   The size of LLMs is typically measured in the number of parameters

-   There are over 50 models \> 1 billion parameters.[^1]

-   Training data may be petabytes in size

## General Purpose

***General Purpose*** means the model can be used to solve a wide
variety of general language problems.

## Pre-trained

***Pre-trained*** means the model has been trained on a large amount of
text data to solve common language problems, such as:

-   text classification and generation
-   question answering
-   summarizing documents
-   generating code
-   translating languages

## Fine-tuned

***Fine-tuned*** means the model has been trained on a **small\***
amount of domain data to solve a specific problem in different domains,
such as:

-   legal
-   medical
-   finance
-   scientific

**\*small** is relative to the amount of data used to pre-train the
model.

## Example LLMs

Here's a non-comprehensive list of some of the largest models:

| Model      | Parameters         | Company    | Open-Source? |
|------------|--------------------|------------|--------------|
| GPT-4      | 1.76 trillion [^2] | OpenAI     | No           |
| GPT-3      | 175 billion        | OpenAI     | No           |
| PaLM 2     | 340 billion        | Google     | Yes          |
| Llama 3    | 70 billion         | Meta       | Yes          |
| mistral-7B | 7 billion          | Mistral AI | Yes          |

## Types of LLMs

-   **Generic Language Models** - predict the next token/word base on
    the training data

## Types of LLMs

-   **Generic Language Models** - predict the next token/word base on
    the training data
-   **Instruction-Tuned** - predict a response to instructions given in
    the input/prompt

## Types of LLMs

-   **Generic Language Models** - predict the next token/word base on
    the training data
-   **Instruction-Tuned** - predict a response to instructions given in
    the input/prompt
-   **Dialouge-Tuned** - type of *Instruction-Tuned* that peforms
    conversational question/answer response

## Selecting a Model

Consider the following when selecting a model:

-   **Size** - larger models (more parameters) typically perform better,
    but require more computational resources (GPU, memory, etc.)
-   **Task** - some models are better suited for specific tasks (e.g.
    summarization, translation, question/answer)
-   **License** - some models are proprietary and require a license

## Ollama

-   `ollama` is an executable that allows you to run LLMs locally on
    your machine

-   Choose from a variety of open-source models and sizes (e.g., llama3,
    mistral-7B, gemma)

-   Each model has an API that allows you to interact with the model
    programmatically

-   Fewer models than HuggingFace, but easier to use

Website: <https://ollama.com/>

## HuggingFace

-   Hosts a huge variety of pre-trained models, including:

    -   LLMs
    -   Image Classification
    -   Object Detection
    -   Text-to-Speech

-   Most models use the
    [transformers](https://huggingface.co/docs/transformers/en/index)
    library in Python

-   Generally harder to use than ollama (especially for beginners)

Website: <https://huggingface.co/>

## Why Run LLMs Locally?

-   **Privacy** - your data stays on your machine
-   **Reproducibility** - models are versioned so you reproduce your results
-   **Cost** - running models locally can be cheaper than using cloud services
-   **Fine-tuning** - you can customize the model to your needs

## Downloading and Running `ollama`

1.  Download the `ollama` executable from the website
    [here](https://ollama.com/download)
2.  Enter `ollama run` in the terminal with the name of the model you
    want to run, for example:

To run the `llama3` model:

``` shell
ollama run llama3
```

## Downloading and Running `ollama`

-   Selecting a model such as `llama3` will download the model and run
    it in on your machine

-   Note that the first time you run a model, it will take some time to
    download the model and its dependencies

-   Beware, some models may be very large and may use all of you
    machine's resources while they are running

## Models and Sizes

You can select different models and sizes based on your needs from the
ollama website.

For example, the [70 billion
parameter](https://ollama.com/library/llama3:70b) llama3 model is 40 GB.

``` shell
ollama run llama3:70b
```

In contrast, the [8 billion
parameter](https://ollama.com/library/llama3:8b) llama3 model is 4.7 GB.

``` shell
ollama run llama3:8b
```

By default, `ollama run llama3` will run the 8 billion parameter model.

## Running `ollama` with a Prompt in the Terminal

Once the model is running, you can interact with it by typing in a
prompt into the terminal.

For example, you can type in a question and the model will generate a
response.

``` text
Q: What are large language models?
```

## Answer

``` text
Large Language Models (LLMs) refer to artificial intelligence (AI) systems that can process and generate
human-like language with unprecedented scale, depth, and quality. These models are typically trained on massive
amounts of text data, often in the form of vast datasets or even entire books, articles, and online content.

The key characteristics of LLMs include:

1. **Large-scale training**: LLMs are trained on enormous datasets, which can range from tens of thousands to
hundreds of millions of examples.
2. **Deep learning architecture**: These models employ complex neural network architectures, such as
transformer-based networks or recurrent neural networks (RNNs), which enable them to capture subtle patterns and
relationships in language.
3. **High-capacity computing resources**: LLMs require significant computational power and memory to train and
process the massive amounts of data they operate on.
4. **Advanced algorithms and techniques**: Researchers use innovative techniques, such as attention mechanisms,
self-attention, and multi-task learning, to improve the models' ability to understand and generate human language.

Some notable examples of LLMs include:

1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google in 2018, BERT is a
widely used LLM that has achieved state-of-the-art results in many natural language processing (NLP) tasks.
2. **RoBERTa (Robustly Optimized BERT Pre-training Approach)**: Released in 2019, RoBERTa is an improved version
of BERT that uses a different optimization approach and achieves better performance on some NLP tasks.
3. ** transformer-based models**: These include language models like T5 (Text-to-Text Transformer), Pegasus
(Perturb-and-Multiply Generative Model for Sequence-to-Sequence Learning), and others, which have been developed
to excel in various NLP tasks.

LLMs have numerous applications across industries, including:

1. **Natural Language Processing (NLP)**: LLMs can be used for language translation, sentiment analysis, question
answering, text summarization, and more.
2. **Content generation**: These models can generate original content, such as articles, stories, or even entire
books, that are coherent and engaging.
3. **Chatbots and virtual assistants**: LLMs can power conversational AI systems that understand and respond to
human language.
4. **Data analysis and summarization**: By analyzing large amounts of text data, LLMs can help identify trends,
patterns, and insights.

The development of Large Language Models has the potential to revolutionize many areas of artificial intelligence,
language understanding, and human-computer interaction.
```

## Caution

-   The response may be slightly different each time you run the model. LLMs are probablistic, not deterministic.

-   Beware: the LLM may give factually incorrect information, so it's
important to verify the information.

## Interacting with `ollama` Programmatically

You can interact with `ollama` programmatically using the API.

We will use an R script to interact with the `ollama` API.

## Link to R Script

The R script can be found [here](R/example-api-request.R).

[^1]: https://matt-rickard.com/a-list-of-1-billion-parameter-llms

[^2]: https://the-decoder.com/gpt-4-has-a-trillion-parameters/
