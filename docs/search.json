[
  {
    "objectID": "Introduction-to-LLMs.html#machine-learning-vs.-artificial-intelligence",
    "href": "Introduction-to-LLMs.html#machine-learning-vs.-artificial-intelligence",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Machine Learning vs. Artificial Intelligence?",
    "text": "Machine Learning vs. Artificial Intelligence?\n\nMachine Learning is sometimes used interchangeably with the term Artificial Intelligence (AI), however machine learning typically refers to algorithms that can learn patterns from data and make predictions based on that data."
  },
  {
    "objectID": "Introduction-to-LLMs.html#machine-learning-vs.-artificial-intelligence-1",
    "href": "Introduction-to-LLMs.html#machine-learning-vs.-artificial-intelligence-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Machine Learning vs. Artificial Intelligence?",
    "text": "Machine Learning vs. Artificial Intelligence?\n\nMachine Learning is sometimes used interchangeably with the term Artificial Intelligence (AI), however machine learning typically refers to algorithms that can learn patterns from data and make predictions based on that data.\nMachine Learning is a subset of AI, which is a broader field that includes other types of algorithms that can perform tasks that typically require human intelligence."
  },
  {
    "objectID": "Introduction-to-LLMs.html#what-are-large-language-models",
    "href": "Introduction-to-LLMs.html#what-are-large-language-models",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "What are Large Language Models?",
    "text": "What are Large Language Models?\n\nLarge Language Models (LLMs) are a subset of Deep Learning, which is a type of Machine Learning."
  },
  {
    "objectID": "Introduction-to-LLMs.html#what-are-large-language-models-1",
    "href": "Introduction-to-LLMs.html#what-are-large-language-models-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "What are Large Language Models?",
    "text": "What are Large Language Models?\n\nLarge Language Models (LLMs) are a subset of Deep Learning, which is a type of Machine Learning.\nLLMs refer to general purpose models that can be pre-trained and the fine-tuned for specific tasks."
  },
  {
    "objectID": "Introduction-to-LLMs.html#what-are-large-language-models-2",
    "href": "Introduction-to-LLMs.html#what-are-large-language-models-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "What are Large Language Models?",
    "text": "What are Large Language Models?\n\nLarge Language Models (LLMs) are a subset of Deep Learning, which is a type of Machine Learning.\nLLMs refer to general purpose models that can be pre-trained and the fine-tuned for specific tasks\nLLMs are a type of generative AI, which creates new content based on patterns in the data it was trained on."
  },
  {
    "objectID": "Introduction-to-LLMs.html#why-llms-are-a-hot-topic",
    "href": "Introduction-to-LLMs.html#why-llms-are-a-hot-topic",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Why LLMs Are a Hot Topic",
    "text": "Why LLMs Are a Hot Topic\n\nRelatively new - GPT-1 (117 million parameters) was released in 2018"
  },
  {
    "objectID": "Introduction-to-LLMs.html#why-llms-are-a-hot-topic-1",
    "href": "Introduction-to-LLMs.html#why-llms-are-a-hot-topic-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Why LLMs Are a Hot Topic",
    "text": "Why LLMs Are a Hot Topic\n\nRelatively new - GPT-1 (117 million parameters) was released in 2018\nPerform well on wide-variety of language tasks that have traditionally been difficult for computers to solve (e.g. question answering, summarization, translation)"
  },
  {
    "objectID": "Introduction-to-LLMs.html#why-llms-are-a-hot-topic-2",
    "href": "Introduction-to-LLMs.html#why-llms-are-a-hot-topic-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Why LLMs Are a Hot Topic",
    "text": "Why LLMs Are a Hot Topic\n\nRelatively new - GPT-1 (117 million parameters) was released in 2018\nPerform well on wide-variety of language tasks that have traditionally been difficult for computers to solve (e.g. question answering, summarization, translation)\nCan be fine-tuned to solve specific problems in different domains"
  },
  {
    "objectID": "Introduction-to-LLMs.html#why-llms-are-a-hot-topic-3",
    "href": "Introduction-to-LLMs.html#why-llms-are-a-hot-topic-3",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Why LLMs Are a Hot Topic",
    "text": "Why LLMs Are a Hot Topic\n\nRelatively new - GPT-1 (117 million parameters) was released in 2018\nPerform well on wide-variety of language tasks that have traditionally been difficult for computers to solve (e.g. question answering, summarization, translation)\nCan be fine-tuned to solve specific problems in different domains\nRecent software innovations (e.g., Transformers) and computational improvements have made training on vast data sets possible"
  },
  {
    "objectID": "Introduction-to-LLMs.html#main-characteristics-of-llms",
    "href": "Introduction-to-LLMs.html#main-characteristics-of-llms",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "3 Main Characteristics of LLMs",
    "text": "3 Main Characteristics of LLMs\n\nLarge"
  },
  {
    "objectID": "Introduction-to-LLMs.html#main-characteristics-of-llms-1",
    "href": "Introduction-to-LLMs.html#main-characteristics-of-llms-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "3 Main Characteristics of LLMs",
    "text": "3 Main Characteristics of LLMs\n\nLarge\nGeneral Purpose"
  },
  {
    "objectID": "Introduction-to-LLMs.html#main-characteristics-of-llms-2",
    "href": "Introduction-to-LLMs.html#main-characteristics-of-llms-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "3 Main Characteristics of LLMs",
    "text": "3 Main Characteristics of LLMs\n\nLarge\nGeneral Purpose\nPre-trained and Fine-tuned"
  },
  {
    "objectID": "Introduction-to-LLMs.html#how-large-are-llms",
    "href": "Introduction-to-LLMs.html#how-large-are-llms",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "How “Large” are LLMs?",
    "text": "How “Large” are LLMs?\n\nThe size of LLMs is typically measured in the number of parameters"
  },
  {
    "objectID": "Introduction-to-LLMs.html#how-large-are-llms-1",
    "href": "Introduction-to-LLMs.html#how-large-are-llms-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "How “Large” are LLMs?",
    "text": "How “Large” are LLMs?\n\nThe size of LLMs is typically measured in the number of parameters\nThere are over 50 models &gt; 1 billion parameters.1\n\nhttps://matt-rickard.com/a-list-of-1-billion-parameter-llms"
  },
  {
    "objectID": "Introduction-to-LLMs.html#how-large-are-llms-2",
    "href": "Introduction-to-LLMs.html#how-large-are-llms-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "How “Large” are LLMs?",
    "text": "How “Large” are LLMs?\n\nThe size of LLMs is typically measured in the number of parameters\nThere are over 50 models &gt; 1 billion parameters.1\nTraining data may be petabytes in size\n\nhttps://matt-rickard.com/a-list-of-1-billion-parameter-llms"
  },
  {
    "objectID": "Introduction-to-LLMs.html#general-purpose",
    "href": "Introduction-to-LLMs.html#general-purpose",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "General Purpose",
    "text": "General Purpose\nGeneral Purpose means the model can be used to solve a wide variety of general language problems, such as:"
  },
  {
    "objectID": "Introduction-to-LLMs.html#general-purpose-1",
    "href": "Introduction-to-LLMs.html#general-purpose-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "General Purpose",
    "text": "General Purpose\nGeneral Purpose means the model can be used to solve a wide variety of general language problems, such as:\n\ntext classification and generation"
  },
  {
    "objectID": "Introduction-to-LLMs.html#general-purpose-2",
    "href": "Introduction-to-LLMs.html#general-purpose-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "General Purpose",
    "text": "General Purpose\nGeneral Purpose means the model can be used to solve a wide variety of general language problems, such as:\n\ntext classification and generation\nquestion answering"
  },
  {
    "objectID": "Introduction-to-LLMs.html#general-purpose-3",
    "href": "Introduction-to-LLMs.html#general-purpose-3",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "General Purpose",
    "text": "General Purpose\nGeneral Purpose means the model can be used to solve a wide variety of general language problems, such as:\n\ntext classification and generation\nquestion answering\nsummarizing documents"
  },
  {
    "objectID": "Introduction-to-LLMs.html#general-purpose-4",
    "href": "Introduction-to-LLMs.html#general-purpose-4",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "General Purpose",
    "text": "General Purpose\nGeneral Purpose means the model can be used to solve a wide variety of general language problems, such as:\n\ntext classification and generation\nquestion answering\nsummarizing documents\ngenerating code"
  },
  {
    "objectID": "Introduction-to-LLMs.html#general-purpose-5",
    "href": "Introduction-to-LLMs.html#general-purpose-5",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "General Purpose",
    "text": "General Purpose\nGeneral Purpose means the model can be used to solve a wide variety of general language problems, such as:\n\ntext classification and generation\nquestion answering\nsummarizing documents\ngenerating code\ntranslating languages"
  },
  {
    "objectID": "Introduction-to-LLMs.html#pre-trained",
    "href": "Introduction-to-LLMs.html#pre-trained",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Pre-trained",
    "text": "Pre-trained\n\nPre-trained means the model has been trained on a large amount of text data to solve common language problems."
  },
  {
    "objectID": "Introduction-to-LLMs.html#pre-trained-1",
    "href": "Introduction-to-LLMs.html#pre-trained-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Pre-trained",
    "text": "Pre-trained\n\nPre-trained means the model has been trained on a large amount of text data to solve common language problems.\nPre-training typically involves training on huge amounts (petabytes) of text data (e.g., web pages scraped from the internet)"
  },
  {
    "objectID": "Introduction-to-LLMs.html#fine-tuned",
    "href": "Introduction-to-LLMs.html#fine-tuned",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Fine-tuned",
    "text": "Fine-tuned\nFine-tuned means the model has been trained on a small* amount of domain data to solve a specific problem in different domains, such as:\n\nfinance (FinGPT, BloombergGPT)\nhealthcare\ncustomer service (chatbots)\ndata extraction\n\n*small is relative to the amount of data used to pre-train the model."
  },
  {
    "objectID": "Introduction-to-LLMs.html#example-llms",
    "href": "Introduction-to-LLMs.html#example-llms",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Example LLMs",
    "text": "Example LLMs\nHere’s a non-comprehensive list of some of the largest models:\n\n\n\nModel\nParameters\nCompany\nOpen-Source?\n\n\n\n\nGPT-4\n1.76 trillion 1\nOpenAI\nNo\n\n\nGPT-3\n175 billion\nOpenAI\nNo\n\n\nPaLM 2\n340 billion\nGoogle\nYes\n\n\nLlama 3\n70 billion\nMeta\nYes\n\n\nmistral-7B\n7 billion\nMistral AI\nYes\n\n\n\nhttps://the-decoder.com/gpt-4-has-a-trillion-parameters/"
  },
  {
    "objectID": "Introduction-to-LLMs.html#types-of-llms",
    "href": "Introduction-to-LLMs.html#types-of-llms",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Types of LLMs",
    "text": "Types of LLMs\n\nGeneric Language Models - predict the next token/word base on the training data"
  },
  {
    "objectID": "Introduction-to-LLMs.html#types-of-llms-1",
    "href": "Introduction-to-LLMs.html#types-of-llms-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Types of LLMs",
    "text": "Types of LLMs\n\nGeneric Language Models - predict the next token/word base on the training data\nInstruction-Tuned - predict a response to instructions given in the input/prompt"
  },
  {
    "objectID": "Introduction-to-LLMs.html#types-of-llms-2",
    "href": "Introduction-to-LLMs.html#types-of-llms-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Types of LLMs",
    "text": "Types of LLMs\n\nGeneric Language Models - predict the next token/word base on the training data\nInstruction-Tuned - predict a response to instructions given in the input/prompt\nDialouge-Tuned - type of Instruction-Tuned that peforms conversational question/answer response"
  },
  {
    "objectID": "Introduction-to-LLMs.html#selecting-a-model",
    "href": "Introduction-to-LLMs.html#selecting-a-model",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Selecting a Model",
    "text": "Selecting a Model\nConsider the following when selecting a model:\n\nSize - larger models (more parameters) typically perform better, but require more computational resources (GPU, memory, etc.)\nTask - some models are better suited for specific tasks (e.g. summarization, translation, question/answer)\nLicense - some models are proprietary and require a license"
  },
  {
    "objectID": "Introduction-to-LLMs.html#ollama",
    "href": "Introduction-to-LLMs.html#ollama",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Ollama",
    "text": "Ollama\n\nollama is an executable that allows you to run LLMs locally on your machine"
  },
  {
    "objectID": "Introduction-to-LLMs.html#ollama-1",
    "href": "Introduction-to-LLMs.html#ollama-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Ollama",
    "text": "Ollama\n\nollama is an executable that allows you to run LLMs locally on your machine\nChoose from a variety of open-source models and sizes (e.g., llama3, mistral-7B, gemma)"
  },
  {
    "objectID": "Introduction-to-LLMs.html#ollama-2",
    "href": "Introduction-to-LLMs.html#ollama-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Ollama",
    "text": "Ollama\n\nollama is an executable that allows you to run LLMs locally on your machine\nChoose from a variety of open-source models and sizes (e.g., llama3, mistral-7B, gemma)\nEach model has an API that allows you to interact with the model programmatically"
  },
  {
    "objectID": "Introduction-to-LLMs.html#ollama-3",
    "href": "Introduction-to-LLMs.html#ollama-3",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Ollama",
    "text": "Ollama\n\nollama is an executable that allows you to run LLMs locally on your machine\nChoose from a variety of open-source models and sizes (e.g., llama3, mistral-7B, gemma)\nEach model has an API that allows you to interact with the model programmatically\nFewer models than HuggingFace, but easier to use\n\nWebsite: https://ollama.com/"
  },
  {
    "objectID": "Introduction-to-LLMs.html#huggingface",
    "href": "Introduction-to-LLMs.html#huggingface",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "HuggingFace",
    "text": "HuggingFace\n\nHosts a huge variety of pre-trained models, including:\n\nLLMs\nImage Classification\nObject Detection\nText-to-Speech\n\nMost models use the transformers library in Python\nGenerally harder to use than ollama (especially for beginners)\n\nWebsite: https://huggingface.co/"
  },
  {
    "objectID": "Introduction-to-LLMs.html#why-run-llms-locally",
    "href": "Introduction-to-LLMs.html#why-run-llms-locally",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Why Run LLMs Locally?",
    "text": "Why Run LLMs Locally?\n\nPrivacy - your data stays on your machine"
  },
  {
    "objectID": "Introduction-to-LLMs.html#why-run-llms-locally-1",
    "href": "Introduction-to-LLMs.html#why-run-llms-locally-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Why Run LLMs Locally?",
    "text": "Why Run LLMs Locally?\n\nPrivacy - your data stays on your machine\nReproducibility - models are versioned so you reproduce your results"
  },
  {
    "objectID": "Introduction-to-LLMs.html#why-run-llms-locally-2",
    "href": "Introduction-to-LLMs.html#why-run-llms-locally-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Why Run LLMs Locally?",
    "text": "Why Run LLMs Locally?\n\nPrivacy - your data stays on your machine\nReproducibility - models are versioned so you reproduce your results\nCost - running models locally can be cheaper than using cloud services"
  },
  {
    "objectID": "Introduction-to-LLMs.html#why-run-llms-locally-3",
    "href": "Introduction-to-LLMs.html#why-run-llms-locally-3",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Why Run LLMs Locally?",
    "text": "Why Run LLMs Locally?\n\nPrivacy - your data stays on your machine\nReproducibility - models are versioned so you reproduce your results\nCost - running models locally can be cheaper than using cloud services\nFine-tuning - you can customize the model to your needs"
  },
  {
    "objectID": "Introduction-to-LLMs.html#downloading-and-running-ollama",
    "href": "Introduction-to-LLMs.html#downloading-and-running-ollama",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Downloading and Running ollama",
    "text": "Downloading and Running ollama\n\nDownload the ollama executable from the website here\nEnter ollama run in the terminal with the name of the model you want to run, for example:\n\nTo run the llama3 model:\nollama run llama3"
  },
  {
    "objectID": "Introduction-to-LLMs.html#downloading-and-running-ollama-1",
    "href": "Introduction-to-LLMs.html#downloading-and-running-ollama-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Downloading and Running ollama",
    "text": "Downloading and Running ollama\n\nSelecting a model such as llama3 will download the model and run it in on your machine"
  },
  {
    "objectID": "Introduction-to-LLMs.html#downloading-and-running-ollama-2",
    "href": "Introduction-to-LLMs.html#downloading-and-running-ollama-2",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Downloading and Running ollama",
    "text": "Downloading and Running ollama\n\nSelecting a model such as llama3 will download the model and run it in on your machine\nNote that the first time you run a model, it will take some time to download the model and its dependencies"
  },
  {
    "objectID": "Introduction-to-LLMs.html#downloading-and-running-ollama-3",
    "href": "Introduction-to-LLMs.html#downloading-and-running-ollama-3",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Downloading and Running ollama",
    "text": "Downloading and Running ollama\n\nSelecting a model such as llama3 will download the model and run it in on your machine\nNote that the first time you run a model, it will take some time to download the model and its dependencies\nBeware, some models may be very large and may use all of you machine’s resources while they are running"
  },
  {
    "objectID": "Introduction-to-LLMs.html#models-and-sizes",
    "href": "Introduction-to-LLMs.html#models-and-sizes",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Models and Sizes",
    "text": "Models and Sizes\nYou can select different models and sizes based on your needs from the ollama website.\nFor example, the 70 billion parameter llama3 model is 40 GB.\nollama run llama3:70b\nIn contrast, the 8 billion parameter llama3 model is 4.7 GB.\nollama run llama3:8b\nBy default, ollama run llama3 will run the 8 billion parameter model."
  },
  {
    "objectID": "Introduction-to-LLMs.html#running-ollama-with-a-prompt-in-the-terminal",
    "href": "Introduction-to-LLMs.html#running-ollama-with-a-prompt-in-the-terminal",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Running ollama with a Prompt in the Terminal",
    "text": "Running ollama with a Prompt in the Terminal\nOnce the model is running, you can interact with it by typing in a prompt into the terminal.\nFor example, you can type in a question and the model will generate a response.\nQ: What are large language models?"
  },
  {
    "objectID": "Introduction-to-LLMs.html#answer",
    "href": "Introduction-to-LLMs.html#answer",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Answer",
    "text": "Answer\nLarge Language Models (LLMs) refer to artificial intelligence (AI) systems that can process and generate\nhuman-like language with unprecedented scale, depth, and quality. These models are typically trained on massive\namounts of text data, often in the form of vast datasets or even entire books, articles, and online content.\n\nThe key characteristics of LLMs include:\n\n1. **Large-scale training**: LLMs are trained on enormous datasets, which can range from tens of thousands to\nhundreds of millions of examples.\n2. **Deep learning architecture**: These models employ complex neural network architectures, such as\ntransformer-based networks or recurrent neural networks (RNNs), which enable them to capture subtle patterns and\nrelationships in language.\n3. **High-capacity computing resources**: LLMs require significant computational power and memory to train and\nprocess the massive amounts of data they operate on.\n4. **Advanced algorithms and techniques**: Researchers use innovative techniques, such as attention mechanisms,\nself-attention, and multi-task learning, to improve the models' ability to understand and generate human language.\n\nSome notable examples of LLMs include:\n\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google in 2018, BERT is a\nwidely used LLM that has achieved state-of-the-art results in many natural language processing (NLP) tasks.\n2. **RoBERTa (Robustly Optimized BERT Pre-training Approach)**: Released in 2019, RoBERTa is an improved version\nof BERT that uses a different optimization approach and achieves better performance on some NLP tasks.\n3. ** transformer-based models**: These include language models like T5 (Text-to-Text Transformer), Pegasus\n(Perturb-and-Multiply Generative Model for Sequence-to-Sequence Learning), and others, which have been developed\nto excel in various NLP tasks.\n\nLLMs have numerous applications across industries, including:\n\n1. **Natural Language Processing (NLP)**: LLMs can be used for language translation, sentiment analysis, question\nanswering, text summarization, and more.\n2. **Content generation**: These models can generate original content, such as articles, stories, or even entire\nbooks, that are coherent and engaging.\n3. **Chatbots and virtual assistants**: LLMs can power conversational AI systems that understand and respond to\nhuman language.\n4. **Data analysis and summarization**: By analyzing large amounts of text data, LLMs can help identify trends,\npatterns, and insights.\n\nThe development of Large Language Models has the potential to revolutionize many areas of artificial intelligence,\nlanguage understanding, and human-computer interaction."
  },
  {
    "objectID": "Introduction-to-LLMs.html#caution",
    "href": "Introduction-to-LLMs.html#caution",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Caution",
    "text": "Caution\n\nThe response may be slightly different each time you run the model. LLMs are probablistic, not deterministic."
  },
  {
    "objectID": "Introduction-to-LLMs.html#caution-1",
    "href": "Introduction-to-LLMs.html#caution-1",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Caution",
    "text": "Caution\n\nThe response may be slightly different each time you run the model. LLMs are probablistic, not deterministic.\nLLMs generate responses based on patterns in the data they were trained on, so the content they produce may no be factually accurate, so it’s important to verify the information."
  },
  {
    "objectID": "Introduction-to-LLMs.html#interacting-with-ollama-programmatically",
    "href": "Introduction-to-LLMs.html#interacting-with-ollama-programmatically",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Interacting with ollama Programmatically",
    "text": "Interacting with ollama Programmatically\nYou can interact with ollama programmatically using the API.\nWe will use an R script to interact with the ollama API."
  },
  {
    "objectID": "Introduction-to-LLMs.html#rollama-r-package",
    "href": "Introduction-to-LLMs.html#rollama-r-package",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "rollama R Package",
    "text": "rollama R Package\n\nrollama is an R package that makes it simple to interact with the ollama API\nv 0.1.0 is available on CRAN (released on May 1, 2024)\nGitHub\nReference Manual"
  },
  {
    "objectID": "Introduction-to-LLMs.html#link-to-r-script",
    "href": "Introduction-to-LLMs.html#link-to-r-script",
    "title": "Introduction to Running LLMs - Locally!",
    "section": "Link to R Script",
    "text": "Link to R Script\nThe R script can be found here."
  }
]